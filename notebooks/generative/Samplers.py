
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Generative_validation.ipynb


from .generative_basics import *
from fcd_torch import *

def is_valid(smiles):
    mol = MolFromSmiles(smiles)
    if mol is not None and mol.GetNumAtoms()>0:
        return MolToSmiles(mol)

def uniqueness_score(mols): return set(mols)

def novelty_score(mols,ref_mols): return set.difference(set(mols),set(ref_mols))

def temperature_dict(temps):
    return {j:[] for j in temps}

hook_function = lambda m,i,o: o

class MolSampler():
    def __init__(self,model_fname:str,text:str='',max_size:int=120,max_mols:int=100,temperature:float=0.7,cpu:bool=False):
        self.model = load_learner(model_fname, cpu=cpu)
        self.text = text
        self.max_size = max_size
        self.temperature = temperature
        self.max_mols = max_mols

@patch
@delegates(MolSampler)
def base_sampler(x:MolSampler, max_size=100, temperature=0.7, return_activations = None,**kwargs):
    '''Base sampler to generate one SMILES using a chemistry model trained with fastai
    temperature : sampling temperature (default = 0.7)
    max_size : maximum size of the SMILES strings (default = 100)'''
    act = getattr(x.model.loss_func, 'activation', noop)
    x.model.model.reset()    # Reset the model
    stop_index = x.model.dls.train.vocab.index(BOS)        # Define the stop token
    idxs = x.model.dls.test_dl([x.text]).items[0].to(x.model.dls.device)
    nums = x.model.dls.train_ds.numericalize     # Numericalize (used to decode)
    accum_idxs = []                   # Store predicted tokens
    x.model.model.eval()
    for _ in range(max_size):
        with torch.no_grad(): preds = x.model.model(idxs[None].to(x.model.dls.device))[0][-1]
        

        res = act(preds)


        if x.temperature != 1.: res.pow_(1 / temperature)
        idx = torch.multinomial(res, 1).item()
        if idx != stop_index:
            accum_idxs.append(idx)
            idxs = TensorText(idxs.new_tensor([idx]))
        else:
            break
    decoded = ''.join([nums.vocab[o] for o in accum_idxs if nums.vocab[o] not in [BOS, PAD]])  # Decode predicted tokens
    return decoded

@patch
@delegates(MolSampler)
def base_sampler_with_activations(x:MolSampler,max_size=100, temperature=0.7, return_activations = None, **kwargs):
    '''Base sampler to generate one SMILES using a chemistry model trained with fastai
    temperature : sampling temperature (default = 0.7)
    max_size : maximum size of the SMILES strings (default = 100)'''
    gather_acts = []
    act = getattr(x.model.loss_func, 'activation', noop)
    x.model.model.reset()    # Reset the model
    stop_index = x.model.dls.train.vocab.index(BOS)        # Define the stop token
    idxs = x.model.dls.test_dl([x.text]).items[0].to(x.model.dls.device)
    nums = x.model.dls.train_ds.numericalize     # Numericalize (used to decode)
    accum_idxs = []                   # Store predicted tokens
    x.model.model.eval()
    layers = x.model.model[0].rnns[2]
    
    
    for _ in range(max_size):
        with torch.no_grad(): 
            with Hook(layers, hook_function, is_forward=False) as hookg:
                with Hook(layers, hook_function, is_forward=True) as hook:
          #  output = model.model.eval()(t[None][None].to(model.dls.device))[0]
                    preds = x.model.model(idxs[None].to(x.model.dls.device))[0][-1]
                    lstm_acts = hook.stored[0]
                    gather_acts.append(lstm_acts.squeeze())


        res = act(preds)

        if x.temperature != 1.: res.pow_(1 / temperature)
        idx = torch.multinomial(res, 1).item()
        if idx != stop_index:
            accum_idxs.append(idx)
            idxs = TensorText(idxs.new_tensor([idx]))
        else:
            break
    decoded = ''.join([nums.vocab[o] for o in accum_idxs if nums.vocab[o] not in [BOS, PAD]])  # Decode predicted tokens
    if return_activations: return decoded, torch.stack(gather_acts)
    return decoded



@patch
@delegates(MolSampler)
def generate_mols(x:MolSampler, max_size=100, max_mols=5, temperature=0.7,return_activations = None,**kwargs):
    '''Generate molecules using a base sampler'''
    if return_activations:  return [x.base_sampler_with_activations(max_size=max_size, temperature=temperature, return_activations=return_activations) for _ in tqdm(range(max_mols))]
        

    return [x.base_sampler(max_size=max_size, temperature=temperature, return_activations=return_activations) for _ in tqdm(range(max_mols))]#list(filter(_is_valid, generated_mols))

@patch
@delegates(MolSampler)
def generative_benchmark(x:MolSampler, temperatures=[], max_size:int=100, max_mols:int=500, iterations:int=3, ref_mols=[], **kwargs):
    '''Runs the generative benchmark for a collection of temperatures. For each temperature value, samples
    max_mols SMILES with size max_size for a given number of iterations
    temperatures : collection of temperatures
    max_size : maximum size of the SMILES strings (default = 100)
    max_mols : maximum number of SMILES to generate (default = 500)
    iterations: number of iterations to run the benchmark (default = 3)'''

    
    if not isinstance(temperatures, list): raise TypeError('Temperatures must be a list.')
    if not isinstance(ref_mols, list): raise TypeError('Must provide a list of reference molecules.')
        
    _validity = temperature_dict(temperatures)
    _novelty = temperature_dict(temperatures)
    _uniqueness = temperature_dict(temperatures)

    for t in temperatures:
        print(f'Sampling with temperature {t}')

        for _ in range(iterations):
            generated_mols = x.generate_mols(max_size=max_size, max_mols=max_mols, temperature=t)
            valid_mols = list(filter(is_valid, generated_mols))

            validity = len(valid_mols)/len(generated_mols)
            novelty = len(novelty_score(valid_mols, ref_mols))/len(valid_mols)
            uniqueness = len(uniqueness_score(valid_mols))/len(valid_mols)
            
            _novelty[t].append(novelty)
            _uniqueness[t].append(uniqueness)
            _validity[t].append(validity)
            
    return list(map(pd.DataFrame, [_validity, _novelty, _uniqueness]))

class FCDBenchmark():
    def __init__(self, mols, ref_mols):
            self.mols = mols
            self.ref_mols = ref_mols
            self.fcd = FCD(device='cuda:0', n_jobs=8)
            self.ref_activations = self.fcd.precalc(self.ref_mols)
            
    def calculate_fcd(self):
        print(f'Running FCD validation with {len(self.mols)} molecules')
        return self.fcd(self.mols, pgen=ref_activations)