
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Concat_model.ipynb
class ConcatDataset(Datasets):
    def __init__(self, x1, x2, y): self.x1,self.x2,self.y = x1,x2,y

    def __len__(self): return len(self.y)

    def __getitem__(self, i): return (self.x1[i], self.x2[i]), self.y[i]

def tabular_nlp_collate(idxs, ds):
    idxs = list(range(64)) #bs = 64
    xb,yb = zip(*[ds[i] for i in idxs])
    x1,x2 = zip(*xb)
    x1 = torch.stack(x1)

    # Calculate the max size in a batch (will need to pad before batch)
    max_len = max([len(x) for x in x2])
    sorted_x2 = sorted(x2,key=len,reverse=True)

    # Padding SMILES sequences
    x2_padded = [pad_chunk(x, pad_idx=1, pad_first=False, seq_len=30, pad_len=max_len) for x in sorted_x2]
    x2 = torch.stack(x2_padded)
    yb = torch.stack(yb)

    return (x1,x2), yb

class ConcatDataLoader():
    def __init__(self, ds, bs=64, shuffle=False, n_workers=8, collate_fn=tabular_nlp_collate):
        self.ds = ds
        self.bs = bs
        self.shuffle = shuffle
        self.n_workers = n_workers = min(n_workers, defaults.cpus)
        self.collate_fn = collate_fn

    def __len__(self): return len(self.ds)//self.bs

    def __iter__(self):
        idxs = L.range(self.ds)
        if self.shuffle: idxs = idxs.shuffle()
        chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)]
        partial(self.collate_fn)
        with ProcessPoolExecutor(self.n_workers) as ex:
            yield from ex.map(self.collate_fn, chunks, ds=self.ds)